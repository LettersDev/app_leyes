import json
import re
import os
from pathlib import Path

# Configuraci√≥n: Directorio de datos
DATA_DIR = Path(__file__).parent.parent / "data"

def clean_text_content(text):
    """
    Aplica las mismas reglas de limpieza que batch_add_laws.py.
    """
    if not text: return ""

    # --- LIMPIEZA AGRESIVA DE ARTEFACTOS DE PDF ---
    
    # 1. Eliminar URLs largas (http/https)
    text = re.sub(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+[^\s]*', '', text)
    
    # 2. Eliminar Timestamps t√≠picos de impresi√≥n (ej: "18/07/2014 2:35 PM")
    text = re.sub(r'\d{1,2}/\d{1,2}/\d{4}\s+\d{1,2}:\d{2}\s+(?:AM|PM|am|pm)', '', text)
    
    # 3. Eliminar frases de generadores de PDF
    text = re.sub(r'Documento sin t√≠tulo', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Page \d+ of \d+', '', text, flags=re.IGNORECASE)
    text = re.sub(r'P√°gina \d+ de \d+', '', text, flags=re.IGNORECASE)

    # 4. Eliminar pies de p√°gina recurrentes (Adaptar seg√∫n se vean m√°s patrones)
    text = re.sub(r'\d+\s+Normas de Orden P√∫blico', '', text, flags=re.IGNORECASE)
    
    # 5. Normalizar espacios (eliminar los espacios dobles que quedaron tras borrar URLs)
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Restaurar saltos de linea b√°sicos si se perdieron por el strip masivo, 
    # aunque aqu√≠ asumimos que el texto ya ven√≠a formateado y solo queremos limpiar basura.
    # Si el texto original ten√≠a saltos \n, el regex \s+ los convirti√≥ en espacio.
    # Para ser seguros con datos existentes, mejor NO colapsar todo a una l√≠nea si ya estaba bien formateado.
    # Pero como el problema reportado es basura insertada, probablemente el formato ya est√© sucio.
    
    # Re-aplicar reglas de formato para asegurar consistencia
    text = re.sub(r'(?<!\n)(\s+)(\d+[¬∞¬∫]\.?)', r'\n\n\2', text)
    text = re.sub(r'(?<!\n)(\s+)(\d+\.)(?=\s[A-Z√Å√â√ç√ì√ö])', r'\n\n\2', text)
    
    numeral_words = [
        'Primero:', 'Segundo:', 'Tercero:', 'Cuarto:', 'Quinto:',
        'Par√°grafo Primero', 'Par√°grafo Segundo', 'Par√°grafo Tercero', 'Par√°grafo √önico'
    ]
    for word in numeral_words:
        pattern = rf'(?<!\n)(\s+)({word})'
        text = re.sub(pattern, r'\n\n\2', text, flags=re.IGNORECASE)

    text = re.sub(r' +', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)

    return text.strip()

def process_file(file_path):
    print(f"üîÑ Procesando: {file_path.name}")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        modified_count = 0
        
        # Iterar sobre las leyes (usualmente es una lista de 1 objeto ley)
        for law in data:
            if 'content' in law and 'articles' in law['content']:
                for article in law['content']['articles']:
                    original_text = article.get('text', '')
                    if not original_text: continue
                    
                    cleaned_text = clean_text_content(original_text)
                    
                    if original_text != cleaned_text:
                        article['text'] = cleaned_text
                        modified_count += 1
        
        if modified_count > 0:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"‚úÖ Guardado. {modified_count} art√≠culos limpiados.")
        else:
            print("‚ú® Sin cambios necesarios.")
            
    except Exception as e:
        print(f"‚ùå Error procesando {file_path.name}: {e}")

def main():
    if not DATA_DIR.exists():
        print(f"‚ùå No se encuentra el directorio data: {DATA_DIR}")
        return

    json_files = list(DATA_DIR.glob("*_full.json"))
    print(f"üîç Encontrados {len(json_files)} archivos JSON de leyes.")
    
    for json_file in json_files:
        process_file(json_file)

    print("\nüèÅ Limpieza completada.")

if __name__ == "__main__":
    main()
